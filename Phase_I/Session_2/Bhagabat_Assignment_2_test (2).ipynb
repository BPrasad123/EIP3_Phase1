{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Bhagabat Assignment 2_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqjbmpgeeHsV",
        "colab_type": "text"
      },
      "source": [
        "Bhagabat Prasad Behera\n",
        "\n",
        "Batch 6\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhW5_ARw6K45",
        "colab_type": "text"
      },
      "source": [
        "# 2A Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUoCWDnUxgov",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzGp-JUnxjmC",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.ibb.co/pdZNB42/backprop22.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEPQby7U_3w3",
        "colab_type": "text"
      },
      "source": [
        "The primary objective of training a neural network is to find the values of its parameters such that the error is minimum. For that, there are [a few techniques](https://stats.stackexchange.com/questions/235862/is-it-possible-to-train-a-neural-network-without-backpropagation) and Backpropagration is the most popular one. In this blog, we will consider a small and simple network for better understanding how backpropagation works step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Bbmy_1DXoG",
        "colab_type": "text"
      },
      "source": [
        "### Overview and Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HuIKxrODp23",
        "colab_type": "text"
      },
      "source": [
        "We will build a network with three layers:\n",
        "* **Input** layer with two input neurons\n",
        "* **Hidden** layer with two neurons\n",
        "* **Output** layer with one neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-1DtlWEQ8s",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.ibb.co/TkqFvgL/backprop23.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7Qt6MxbElzh",
        "colab_type": "text"
      },
      "source": [
        "### Weight Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_hrDpaKE33t",
        "colab_type": "text"
      },
      "source": [
        "Since the goal is to minimize the the prediction error, we will initialize the model weights to [small random positive values near to 0 and less than 1](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/). Then the backpropagation technique will use gradient descent optimization to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
        "\n",
        "Our initial weights will be as : `w1 = 0.22`, `w2 = 0.09`, `w3 = 0.05`, `w4 = 0.11`, `w5 = 0.18` and `w6 = 0.13`. The bias is initialized with zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADpHcjIeO6xE",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.ibb.co/qszJsHR/backprop8.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ7JmZAjPDcg",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL5IhW20PMHA",
        "colab_type": "text"
      },
      "source": [
        "Let us take two input values and one output value. The values are as following:\n",
        "\n",
        "inputs = [2, 3]\n",
        "\n",
        "output = [1].\n",
        "\n",
        "![](https://i.ibb.co/G5Z3Pn8/dataset.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUdi1gZKPZDo",
        "colab_type": "text"
      },
      "source": [
        "### Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUAgzUvzRGS6",
        "colab_type": "text"
      },
      "source": [
        "We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n",
        "\n",
        "![](https://i.ibb.co/wYBC1LN/backprop9.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taWTpBLLgLk-",
        "colab_type": "text"
      },
      "source": [
        "$\\begin{bmatrix}2 \\ 3\\end{bmatrix} $. $\\begin{bmatrix}0.22 & 0.05 \\\\ 0.09 & 0.11\\end{bmatrix}$ = $\\begin{bmatrix}0.71\\ 0.43\\end{bmatrix} $\n",
        "\n",
        "$\\begin{bmatrix}0.71\\ 0.43\\end{bmatrix} $.$\\begin{bmatrix}0.18\\\\ 0.13\\end{bmatrix} $ = [0.184]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGMG_ngsiDb1",
        "colab_type": "text"
      },
      "source": [
        "2 x 0.22 + 3 x 0.9 = 0.71\n",
        "\n",
        "2 x 0.05 + 3 x 0.11 = 0.43\n",
        "\n",
        "0.71 x 0.18 + 0.43 x 0.13 = 0.184"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4uh5GP1ie8X",
        "colab_type": "text"
      },
      "source": [
        "### Calculating Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IejxfOQ_ir8H",
        "colab_type": "text"
      },
      "source": [
        "Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or prediction, is not even close to actual output. We can calculate the difference or the error as following.\n",
        "\n",
        "![](https://i.ibb.co/xXhGRWb/backprop10.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3E4D3Tn5qZ-",
        "colab_type": "text"
      },
      "source": [
        "### Reducing Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ6L-g5L6DL0",
        "colab_type": "text"
      },
      "source": [
        "Our main goal of the training is to reduce the error or the difference between prediction and actual output. Since actual output is constant, “not changing”, and we cannot change the input values, the only way to reduce the error is to change prediction value. The question now is, how to change prediction value?\n",
        "\n",
        "By decomposing prediction into its basic elements we can find that weights are the variable elements in the network affecting prediction value. In other words, in order to change prediction value, we need to change weights values.\n",
        "\n",
        "![](https://i.ibb.co/gg5PVQ7/backprop11.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NGS0XVN-hFz",
        "colab_type": "text"
      },
      "source": [
        "The question now is **how to change\\update the weights value so that the error is reduced?**\n",
        "The answer is **Backpropagation**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNFXZ9Pu-_rI",
        "colab_type": "text"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN_F5L3l_CZc",
        "colab_type": "text"
      },
      "source": [
        "**Backpropagation**, short for “backward propagation of errors”, is a mechanism used to update the weights using gradient descent. It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n",
        "\n",
        "**Gradient descent** is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n",
        "\n",
        "![](https://i.ibb.co/LxXx1KJ/backprop12.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBTsFbdqC-sc",
        "colab_type": "text"
      },
      "source": [
        "For example, to update `w5`, we take the current `w5` and subtract the partial derivative of error function with respect to `w5`. Optionally, we multiply the derivative of the error function by a selected number to make sure that the new updated weight is minimizing the error function; this number is called learning rate.\n",
        "\n",
        "![](https://i.ibb.co/cXSTVDm/backprop13.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFVYWIoEVsl",
        "colab_type": "text"
      },
      "source": [
        "The derivation of the error function is evaluated by applying the chain rule as following\n",
        "\n",
        "![](https://i.ibb.co/pLyMY3J/backprop14.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWLU3aa6OtcW",
        "colab_type": "text"
      },
      "source": [
        "So to update `w5` we can apply the following formula:\n",
        "\n",
        "![](https://i.ibb.co/HHWmZPS/backprop15.jpg)\n",
        "\n",
        "Similarly, we can derive the update formula for w6 and any other weights existing between the output and the hidden layer.\n",
        "\n",
        "![](https://i.ibb.co/gwLfhDh/backprop16.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-wG0pTmQMDb",
        "colab_type": "text"
      },
      "source": [
        "However, when moving backward to update `w1`, `w2`, `w3` and `w4` existing between input and hidden layer, the partial derivative for the error function with respect to `w1`, for example, will be as following.\n",
        "\n",
        "![](https://i.ibb.co/s2X9rkQ/backprop17.jpg)\n",
        "\n",
        "We can find the update formula for the remaining weights w2, w3 and w4 in the same way. In summary, the update formulas for all weights will be as following:\n",
        "\n",
        "![](https://i.ibb.co/6nY7nyr/backprop20.jpg)\n",
        "\n",
        "We can rewrite the update formulas in matrices as following:\n",
        "\n",
        "![](https://i.ibb.co/nkQwRZs/backprop19.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW2rlQXQ_sKi",
        "colab_type": "text"
      },
      "source": [
        "### Backward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEWg0xQydN_G",
        "colab_type": "text"
      },
      "source": [
        "Using derived formulas we can find the **new weights**.\n",
        "\n",
        "**Learning rate**: is a hyperparameter which means that we need to manually guess its value. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pXtuOebeGtS",
        "colab_type": "text"
      },
      "source": [
        "∆ = 0.184 - 1 = -0.816\n",
        "\n",
        "*Delta = prediction - actual*\n",
        "\n",
        "a = 0.05 \n",
        "\n",
        "*We choose a small number*\n",
        "\n",
        "\n",
        "$\\begin{bmatrix}w5 \\\\ w6\\end{bmatrix} $ = $\\begin{bmatrix}0.18 \\\\ 0.13\\end{bmatrix} $ - 0.05(0.816) $\\begin{bmatrix}0.71 \\\\ 0.43\\end{bmatrix} $ = $\\begin{bmatrix}0.18 \\\\ 0.13\\end{bmatrix} $ - $\\begin{bmatrix}-0.029 \\\\ -0.017\\end{bmatrix} $ = $\\begin{bmatrix}0.21 \\\\ 0.15\\end{bmatrix} $\n",
        "\n",
        "$\\begin{bmatrix}w1 & w3 \\\\ w2 & w4\\end{bmatrix}$ = $\\begin{bmatrix}0.22 & 0.05 \\\\ 0.09 & 0.11\\end{bmatrix}$ - 0.05(-0.816)$\\begin{bmatrix}2\\\\ 3\\end{bmatrix} $.$\\begin{bmatrix}0.18\\ 0.13\\end{bmatrix} $ = $\\begin{bmatrix}0.22 & 0.05 \\\\ 0.09 & 0.11\\end{bmatrix}$ - $\\begin{bmatrix}-0.015 & -0.01 \\\\ -0.022 & -0.016\\end{bmatrix}$ = $\\begin{bmatrix}0.23 & 0.06 \\\\ 0.11 & 0.13\\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rebZWdDIqejN",
        "colab_type": "text"
      },
      "source": [
        "Now, using the **new weights** we will repeat the forward pass:\n",
        "\n",
        "![](https://i.ibb.co/N6N2qCr/newbackprop.jpg)\n",
        "\n",
        "$\\begin{bmatrix}2 \\ 3\\end{bmatrix} $. $\\begin{bmatrix}0.23 & 0.06 \\\\ 0.11 & 0.13\\end{bmatrix}$ = $\\begin{bmatrix}0.79\\ 0.51\\end{bmatrix} $\n",
        "\n",
        "$\\begin{bmatrix}0.79\\ 0.51\\end{bmatrix} $.$\\begin{bmatrix}0.21\\\\ 0.15\\end{bmatrix} $ = [0.242]\n",
        "\n",
        "2 x 0.23 + 3 x 0.11 = 0.79 \n",
        "\n",
        "2 x 0.06+ 3 x 0.13 = 0.51\n",
        "\n",
        "0.79 x 0.21 + 0.51 x 0.15 = 0.242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyIsyh4c0NZg",
        "colab_type": "text"
      },
      "source": [
        "We can notice that the prediction **0.242** is a little bit closer to actual output than the previously predicted one 0.191. We can repeat the same process of backward and forward pass until error is close or equal to zero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjpjlKlR6881",
        "colab_type": "text"
      },
      "source": [
        "# 2B Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHpjvsXN6_87",
        "colab_type": "text"
      },
      "source": [
        "The following section will implement the above backpropagation calculation in Python.\n",
        "\n",
        "To start with we had:\n",
        "\n",
        "inputs = [2, 3]\n",
        "\n",
        "output = [1]\n",
        "\n",
        "weights = [w1, w2, w3, w4, w5, w6] = [0.22, 0.09, 0.05, 0.11, 0.18, 0.13]\n",
        "\n",
        "We will import numpy library to do the matrix opration to do the following:\n",
        "* Forward pass to predict the output\n",
        "* Calculate the difference between prediction and actual output. \n",
        "* Update the weights considering the derived formula from assignment 2A part\n",
        "* Forward pass again to predict output with updated weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1IaG_qgeBv",
        "colab_type": "text"
      },
      "source": [
        "### Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46hOsUQ9LxG",
        "colab_type": "code",
        "outputId": "485a0b75-766c-4a1e-f720-af290ca32f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Importing required library for matrix operation\n",
        "import numpy as np\n",
        "\n",
        "# initializing the weights with random values same as considered in assignment 2A\n",
        "w1 = 0.22\n",
        "w2 = 0.09\n",
        "w3 = 0.05\n",
        "w4 = 0.11\n",
        "w5 = 0.18\n",
        "w6 = 0.13\n",
        "\n",
        "# define input array with corresponding values\n",
        "inputs = np.array([[2,3]], dtype=np.float32)\n",
        "\n",
        "# define array of weights for inputs to hidden layer\n",
        "weights_1 = np.array([[w1, w3], [w2, w4]], dtype=np.float32)\n",
        "\n",
        "# calculate the values at the nodes in hidden layer\n",
        "h_val = inputs.dot(weights_1)\n",
        "\n",
        "# define array for weights for inputs to output layer\n",
        "weights_2 = np.array([w5, w6], dtype=np.float32)\n",
        "\n",
        "# calculate the output value\n",
        "output = h_val.dot(weights_2)\n",
        "\n",
        "print('output:', output)\n",
        "print(output.dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output: [0.18370001]\n",
            "float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIH3z06-HP7",
        "colab_type": "text"
      },
      "source": [
        "**Comparison:**\n",
        "\n",
        "In the manual calculation we had rounded the calculated output value to three decimal places. Hence, the output value, as mentioned in assignment 2A is 0.184.\n",
        "\n",
        "However, in the code, we are not rounding any number. And as per the set data type, all the matrix elements including the output value here are considered with 32 decimal places."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBcMJfu9rjfi",
        "colab_type": "text"
      },
      "source": [
        "### Calculate Delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjQh1Ypprl_K",
        "colab_type": "code",
        "outputId": "a8df25f4-41bc-4d8d-d3be-931dd70eb2d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the difference between the predicted value and actual output value\n",
        "delta = output[0] - 1\n",
        "\n",
        "# set the value of learning rate\n",
        "a = 0.05\n",
        "\n",
        "print('delta: ', delta)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "delta:  -0.8162999898195267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v29B1w9JUrm",
        "colab_type": "text"
      },
      "source": [
        "### Backward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGQQnNq_sCnk",
        "colab_type": "code",
        "outputId": "5b9f9975-593c-4485-da60-43d1e919db24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# considering the values of delta and learning rate, calculate the new values of weights of inputs to the output layer\n",
        "\n",
        "weights_2_u = np.array([0.18, 0.13], dtype=np.float32) - (delta * a) * h_val\n",
        "print('Updated weights for inputs to output layer: \\n', weights_2_u, '\\n')\n",
        "\n",
        "# similarly calculate the new values of weights of inputs to the hidden layer\n",
        "weights_1_u = weights_1 - np.multiply(((delta * a) * inputs.T), weights_2.T)\n",
        "print('Updated weights for inputs to hidden layer: \\n', weights_1_u)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated weights for inputs to output layer: \n",
            " [[0.20897865 0.14755045]] \n",
            "\n",
            "Updated weights for inputs to hidden layer: \n",
            " [[0.2346934  0.0606119 ]\n",
            " [0.1120401  0.12591785]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mChbZDpGCv-C",
        "colab_type": "text"
      },
      "source": [
        "**Comparison**\n",
        "\n",
        "The updated weights from the manual calculation (in assignment 2A):\n",
        "\n",
        "$\\begin{bmatrix}w1 \\\\ w2 \\end{bmatrix}$ = $\\begin{bmatrix}0.21 \\\\ 0.15\\end{bmatrix}$\n",
        "\n",
        "$\\begin{bmatrix}w1 & w3 \\\\ w2 & w4\\end{bmatrix}$ = $\\begin{bmatrix}0.23 & 0.06 \\\\ 0.11 & 0.13\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAPLM1siww-J",
        "colab_type": "text"
      },
      "source": [
        "### Forward pass again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn3_lElWw0vs",
        "colab_type": "code",
        "outputId": "5b6d53f4-c473-4719-cebc-7c5a86e046a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# calculate the updated values at the hidden nodes\n",
        "h_val_u = inputs.dot(weights_1_u)\n",
        "\n",
        "# calculate the output value with updated weights\n",
        "output_u = h_val_u.dot(weights_2_u.T)\n",
        "\n",
        "print(\"Updated values at hidden nodes: \\n\", h_val_u)\n",
        "\n",
        "print(\"\\nUpdated output value: \", output_u)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated values at hidden nodes: \n",
            " [[0.80550706 0.49897736]]\n",
            "\n",
            "Updated output value:  [[0.24195811]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7DZRCzTMSEo",
        "colab_type": "text"
      },
      "source": [
        "**Comparison**\n",
        "\n",
        "Point to note here is that the calculated values at the nodes are slightly different from the corresponding ones from manual calculation because of the difference in decimal places considered.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJZMgOUjM-Ob",
        "colab_type": "text"
      },
      "source": [
        "### Outputs from manual calculation and Python implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWNAmWtDNJfa",
        "colab_type": "text"
      },
      "source": [
        "Output from manual calculation (assignment 2A): **0.242**\n",
        "\n",
        "Output from python implementation (assignment 2B): **0.24195811**\n",
        "\n",
        "Interestingly, both the values are pretty close. Of course, the value from python implementation is more accurate, given that all the numbers were considered with 32 decimal places."
      ]
    }
  ]
}