Bhagabat Prasad Behera
bhagabat.prasad123@gmail.com
Batch 6

# Activation Function

A neuron simply takes weighted sum of the inputs and then transforms it using an activated function. The type of activation function determines how easy it is to train the model and how much complex mapping function it can capture. 

![alt](https://i.ibb.co/kX5WPwX/Activation-function.png)

A pure linear function makes training easier but fails to capture complex mapping. And it is otherwise in case of a pure non-linear function with additional drawbacks like limited sensitivity and saturation. A function like ReLU is a blend of both linear and non-linear functions giving better result.